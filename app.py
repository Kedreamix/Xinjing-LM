import os
from threading import Thread
import gradio as gr
import torch
from modelscope import AutoModelForCausalLM, AutoTokenizer
from transformers import TextIteratorStreamer

# å‚æ•°é…ç½®
MAX_MAX_NEW_TOKENS = 8192
DEFAULT_MAX_NEW_TOKENS = 1024
MAX_INPUT_TOKEN_LENGTH = int(os.getenv("MAX_INPUT_TOKEN_LENGTH", "4096"))

# ä½¿ç”¨å¿ƒå¢ƒå¤§æ¨¡å‹ï¼ˆXinjing-LMï¼‰
model_id = "Kedreamix/Xinjing-LM"
trust_remote_code = True
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map="auto",
                                                trust_remote_code=trust_remote_code)
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=trust_remote_code)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.use_default_system_prompt = False
    
# ç”Ÿæˆå‡½æ•°ï¼Œå¤„ç†æ¨ç†è¿‡ç¨‹
def generate(
        message: str,
        chat_history: list[tuple[str, str]],
        system_prompt: str,
        max_new_tokens: int = 1024,
        temperature: float = 0.6,
        top_p: float = 0.9,
        top_k: int = 50,
        repetition_penalty: float = 1.2,
) -> str:
    # å¼ºåˆ¶æ¨¡å‹å¤šæ€è€ƒ
    prompt_template = f"<|im_start|>user\n{message}<|im_end|>\n<|im_start|>assistant\n<think>"
    
    # æ·»åŠ å†å²å¯¹è¯å¤„ç†
    for user_msg, assistant_msg in chat_history:
        prompt_template = f"<|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant\n<think>{assistant_msg}" + prompt_template
    
    print(prompt_template)
    inputs = tokenizer(prompt_template, return_tensors="pt").to(model.device)

    streamer = TextIteratorStreamer(tokenizer, timeout=6.0, skip_prompt=True, skip_special_tokens=True)
    generate_kwargs = dict(
        input_ids=inputs.input_ids,
        streamer=streamer,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        top_p=top_p,
        top_k=top_k,
        temperature=temperature,
        repetition_penalty=repetition_penalty,
    )

    t = Thread(target=model.generate, kwargs=generate_kwargs)
    t.start()

    # ç­‰å¾…ç”Ÿæˆç»“æœå¹¶è¿”å›
    outputs = []
    for text in streamer:
        outputs.append(text)
        yield "".join(outputs)

# Gradio ç•Œé¢è®¾ç½®
chat_interface = gr.ChatInterface(
    fn=generate,
    additional_inputs=[
        gr.Textbox(label="ç³»ç»Ÿæç¤º", lines=6),
        gr.Slider(
            label="æœ€å¤§ç”Ÿæˆä»¤ç‰Œ",
            minimum=1,
            maximum=MAX_MAX_NEW_TOKENS,
            step=1,
            value=DEFAULT_MAX_NEW_TOKENS,
        ),
        gr.Slider(
            label="æ¸©åº¦ (Temperature)",
            minimum=0.1,
            maximum=4.0,
            step=0.1,
            value=0.6,
        ),
        gr.Slider(
            label="Top-p (æ ¸é‡‡æ ·)",
            minimum=0.05,
            maximum=1.0,
            step=0.05,
            value=0.9,
        ),
        gr.Slider(
            label="Top-k",
            minimum=1,
            maximum=1000,
            step=1,
            value=50,
        ),
        gr.Slider(
            label="é‡å¤æƒ©ç½š",
            minimum=1.0,
            maximum=2.0,
            step=0.05,
            value=1.2,
        ),
    ],
    stop_btn=None,
    examples=[
        ["è¯·åˆ†äº«ä¸€äº›åº”å¯¹ç„¦è™‘çš„æ–¹æ³•ã€‚"],
        ["ç»™æˆ‘ä¸€äº›ç¼“è§£å‹åŠ›çš„å»ºè®®ã€‚"],
        ["å¦‚ä½•ç®¡ç†æƒ…ç»ªä»¥æå‡å¿ƒç†å¥åº·ï¼Ÿ"],
        ["æœ‰å“ªäº›æ”¾æ¾çš„å†¥æƒ³æŠ€å·§å¯ä»¥å¸®åŠ©å‡å‹ï¼Ÿ"],
        ["æˆ‘è¯¥å¦‚ä½•é¢å¯¹è´Ÿé¢æƒ…ç»ªï¼Ÿ"]
    ],
)

# é¡µé¢è®¾è®¡
with gr.Blocks(css="style.css") as demo:
    gr.Markdown(
        """<p align="center"><img src="https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png" style="height: 80px"/><p>""")
    gr.Markdown("""<center><font size=8>Xinjing-LM å¿ƒç†å¥åº·åŠ©æ‰‹ ğŸ‘¾</center>""")
    gr.Markdown(
        """<center><font size=4>Xinjing-LM æ˜¯ä¸€æ¬¾ä¸“æ³¨äºå¿ƒç†å¥åº·æ”¯æŒçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿæä¾›å‹åŠ›ç®¡ç†ã€ç„¦è™‘ç¼“è§£ã€æƒ…ç»ªè°ƒèŠ‚ç­‰æ–¹é¢çš„å»ºè®®ã€‚</center>""")

    # æ¸²æŸ“ChatInterface
    chat_interface.render()

if __name__ == "__main__":
    demo.queue().launch()